{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SEG_Facies_Classification.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nshea3/SEG_Facies_Classification/blob/master/SEG_Facies_Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Yl6Z7Ytsfgk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "data = pd.read_csv('https://raw.githubusercontent.com/seg/2016-ml-contest/master/training_data.csv')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jv1j1ztw_GoL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import KFold , StratifiedKFold\n",
        "from sklearn.metrics import confusion_matrix, f1_score\n",
        "from sklearn import preprocessing\n",
        "from sklearn.model_selection import LeavePGroupsOut\n",
        "from sklearn.multiclass import OneVsOneClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from scipy.signal import medfilt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c9QpsHDtsrYl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "6a9616a6-263a-49ca-f506-310fe92875ff"
      },
      "source": [
        "#Load Data\n",
        "data = pd.read_csv('https://raw.githubusercontent.com/seg/2016-ml-contest/master/facies_vectors.csv')\n",
        "\n",
        "# Parameters\n",
        "feature_names = ['GR', 'ILD_log10', 'DeltaPHI', 'PHIND', 'PE', 'NM_M', 'RELPOS']\n",
        "facies_names = ['SS', 'CSiS', 'FSiS', 'SiSh', 'MS', 'WS', 'D', 'PS', 'BS']\n",
        "facies_colors = ['#F4D03F', '#F5B041','#DC7633','#6E2C00', '#1B4F72','#2E86C1', '#AED6F1', '#A569BD', '#196F3D']\n",
        "\n",
        "# Store features and labels\n",
        "X = data[feature_names].values \n",
        "y = data['Facies'].values \n",
        "\n",
        "# Store well labels and depths\n",
        "well = data['Well Name'].values\n",
        "depth = data['Depth'].values\n",
        "\n",
        "# Fill 'PE' missing values with mean\n",
        "imp = preprocessing.Imputer(missing_values='NaN', strategy='mean', axis=0)\n",
        "imp.fit(X)\n",
        "X = imp.transform(X)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:66: DeprecationWarning: Class Imputer is deprecated; Imputer was deprecated in version 0.20 and will be removed in 0.22. Import impute.SimpleImputer from sklearn instead.\n",
            "  warnings.warn(msg, category=DeprecationWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ax22uIvcstbT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Feature windows concatenation function\n",
        "def augment_features_window(X, N_neig):\n",
        "    \n",
        "    # Parameters\n",
        "    N_row = X.shape[0]\n",
        "    N_feat = X.shape[1]\n",
        "\n",
        "    # Zero padding\n",
        "    X = np.vstack((np.zeros((N_neig, N_feat)), X, (np.zeros((N_neig, N_feat)))))\n",
        "\n",
        "    # Loop over windows\n",
        "    X_aug = np.zeros((N_row, N_feat*(2*N_neig+1)))\n",
        "    for r in np.arange(N_row)+N_neig:\n",
        "        this_row = []\n",
        "        for c in np.arange(-N_neig,N_neig+1):\n",
        "            this_row = np.hstack((this_row, X[r+c]))\n",
        "        X_aug[r-N_neig] = this_row\n",
        "\n",
        "    return X_aug\n",
        "\n",
        "\n",
        "# Feature gradient computation function\n",
        "def augment_features_gradient(X, depth):\n",
        "    \n",
        "    # Compute features gradient\n",
        "    d_diff = np.diff(depth).reshape((-1, 1))\n",
        "    d_diff[d_diff==0] = 0.001\n",
        "    X_diff = np.diff(X, axis=0)\n",
        "    X_grad = X_diff / d_diff\n",
        "        \n",
        "    # Compensate for last missing value\n",
        "    X_grad = np.concatenate((X_grad, np.zeros((1, X_grad.shape[1]))))\n",
        "    \n",
        "    return X_grad\n",
        "\n",
        "\n",
        "# Feature augmentation function\n",
        "def augment_features(X, well, depth, N_neig=1):\n",
        "    \n",
        "    # Augment features\n",
        "    X_aug = np.zeros((X.shape[0], X.shape[1]*(N_neig*2+2)))\n",
        "    for w in np.unique(well):\n",
        "        w_idx = np.where(well == w)[0]\n",
        "        X_aug_win = augment_features_window(X[w_idx, :], N_neig)\n",
        "        X_aug_grad = augment_features_gradient(X[w_idx, :], depth[w_idx])\n",
        "        X_aug[w_idx, :] = np.concatenate((X_aug_win, X_aug_grad), axis=1)\n",
        "    \n",
        "    # Find padded rows\n",
        "    padded_rows = np.unique(np.where(X_aug[:, 0:7] == np.zeros((1, 7)))[0])\n",
        "    \n",
        "    return X_aug, padded_rows"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2PJXjTrDsxa3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_aug, padded_rows = augment_features(X, well, depth)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dbUKAaW4sz6v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def preprocess():\n",
        "    \n",
        "    # Preprocess data to use in model\n",
        "    X_train_aux = []\n",
        "    X_test_aux = []\n",
        "    y_train_aux = []\n",
        "    y_test_aux = []\n",
        "    \n",
        "    # For each data split\n",
        "    split = split_list[5]\n",
        "        \n",
        "    # Remove padded rows\n",
        "    split_train_no_pad = np.setdiff1d(split['train'], padded_rows)\n",
        "\n",
        "    # Select training and validation data from current split\n",
        "    X_tr = X_aug[split_train_no_pad, :]\n",
        "    X_v = X_aug[split['val'], :]\n",
        "    y_tr = y[split_train_no_pad]\n",
        "    y_v = y[split['val']]\n",
        "\n",
        "    # Select well labels for validation data\n",
        "    well_v = well[split['val']]\n",
        "\n",
        "    # Feature normalization\n",
        "    scaler = preprocessing.RobustScaler(quantile_range=(25.0, 75.0)).fit(X_tr)\n",
        "    X_tr = scaler.transform(X_tr)\n",
        "    X_v = scaler.transform(X_v)\n",
        "        \n",
        "    X_train_aux.append( X_tr )\n",
        "    X_test_aux.append( X_v )\n",
        "    y_train_aux.append( y_tr )\n",
        "    y_test_aux.append (  y_v )\n",
        "    \n",
        "    X_train = np.concatenate( X_train_aux )\n",
        "    X_test = np.concatenate ( X_test_aux )\n",
        "    y_train = np.concatenate ( y_train_aux )\n",
        "    y_test = np.concatenate ( y_test_aux )\n",
        "    \n",
        "    return X_train , X_test , y_train , y_test"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qbs7bPYEs_v6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.ensemble import  RandomForestClassifier, VotingClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import BernoulliNB\n",
        "from sklearn.pipeline import make_pipeline, make_union\n",
        "from sklearn.preprocessing import FunctionTransformer\n",
        "import xgboost as xgb\n",
        "from xgboost.sklearn import  XGBClassifier"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JSi9gc6MtCTX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_and_test(X_tr, y_tr, X_v, well_v, clf):\n",
        "    \n",
        "    # Feature normalization\n",
        "    scaler = preprocessing.RobustScaler(quantile_range=(25.0, 75.0)).fit(X_tr)\n",
        "    X_tr = scaler.transform(X_tr)\n",
        "    X_v = scaler.transform(X_v)\n",
        "    \n",
        "    clf.fit(X_tr, y_tr)\n",
        "    \n",
        "    # Test classifier\n",
        "    y_v_hat = clf.predict(X_v)\n",
        "    \n",
        "    # Clean isolated facies for each well\n",
        "    for w in np.unique(well_v):\n",
        "        y_v_hat[well_v==w] = medfilt(y_v_hat[well_v==w], kernel_size=5)\n",
        "    \n",
        "    return y_v_hat"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l8EGh1wLtFp1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_data = pd.read_csv('https://raw.githubusercontent.com/seg/2016-ml-contest/master/validation_data_nofacies.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R2eZebobtRpM",
        "colab_type": "code",
        "outputId": "9b359bff-8e10-4853-a3b1-4ebb053c0be0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "#Load testing data\n",
        "\n",
        "    # Train classifier\n",
        "    #clf = make_pipeline(make_union(VotingClassifier([(\"est\", ExtraTreesClassifier(criterion=\"gini\", max_features=1.0, n_estimators=500))]), FunctionTransformer(lambda X: X)), XGBClassifier(learning_rate=0.73, max_depth=10, min_child_weight=10, n_estimators=500, subsample=0.27))\n",
        "    #clf =  make_pipeline( KNeighborsClassifier(n_neighbors=5, weights=\"distance\") ) \n",
        "    #clf = make_pipeline(MaxAbsScaler(),make_union(VotingClassifier([(\"est\", RandomForestClassifier(n_estimators=500))]), FunctionTransformer(lambda X: X)),ExtraTreesClassifier(criterion=\"entropy\", max_features=0.0001, n_estimators=500))\n",
        "    # * clf = make_pipeline( make_union(VotingClassifier([(\"est\", BernoulliNB(alpha=60.0, binarize=0.26, fit_prior=True))]), FunctionTransformer(lambda X: X)),RandomForestClassifier(n_estimators=500))\n",
        "\n",
        "# # Prepare training data\n",
        "# X_tr = X\n",
        "# y_tr = y\n",
        "\n",
        "# # Augment features\n",
        "# X_tr, padded_rows = augment_features(X_tr, well, depth)\n",
        "\n",
        "# # Removed padded rows\n",
        "# X_tr = np.delete(X_tr, padded_rows, axis=0)\n",
        "# y_tr = np.delete(y_tr, padded_rows, axis=0) \n",
        "\n",
        "# Prepare test data\n",
        "well_ts = test_data['Well Name'].values\n",
        "depth_ts = test_data['Depth'].values\n",
        "X_ts = test_data[feature_names].values\n",
        "\n",
        "\n",
        "    \n",
        "y_pred = []\n",
        "print('.' * 20)\n",
        "for seed in range(20):\n",
        "    np.random.seed(seed)\n",
        "\n",
        "    # Make training data.\n",
        "    X_train, padded_rows = augment_features(X, well, depth)\n",
        "    y_train = y\n",
        "    X_train = np.delete(X_train, padded_rows, axis=0)\n",
        "    y_train = np.delete(y_train, padded_rows, axis=0) \n",
        "\n",
        "    # Train classifier  \n",
        "    clf = make_pipeline(XGBClassifier(learning_rate=0.12,\n",
        "                                      max_depth=3,\n",
        "                                      min_child_weight=10,\n",
        "                                      n_estimators=150,\n",
        "                                      seed=seed,\n",
        "                                      colsample_bytree=0.9))\n",
        "\n",
        "    # Make blind data.\n",
        "    X_test, _ = augment_features(X_ts, well_ts, depth_ts)\n",
        "\n",
        "    # Train and test.\n",
        "    y_ts_hat = train_and_test(X_train, y_train, X_test, well_ts, clf)\n",
        "    \n",
        "    # Collect result.\n",
        "    y_pred.append(y_ts_hat)\n",
        "    print('|', end='')\n",
        "    \n",
        "np.save('LA_Team_100_realizations.npy', y_pred)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "....................\n",
            "||||||||||||||||||||"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2BDIRiQ4yMus",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "PRED = pd.read_csv('https://raw.githubusercontent.com/seg/2016-ml-contest/master/prediction_depths.csv')\n",
        "PRED.set_index([\"Well Name\", \"Depth\"], inplace=True)\n",
        "PRED.head()\n",
        "\n",
        "TRUE = pd.read_csv('https://raw.githubusercontent.com/seg/2016-ml-contest/master/blind_stuart_crawford_core_facies.csv')\n",
        "TRUE.rename(columns={'Depth.ft': 'Depth'}, inplace=True)\n",
        "TRUE.rename(columns={'WellName': 'Well Name'}, inplace=True)\n",
        "TRUE.set_index([\"Well Name\", \"Depth\"], inplace=True)\n",
        "TRUE.head()\n",
        "\n",
        "\n",
        "def get_accuracies(y_preds):\n",
        "    \"\"\"\n",
        "    Get the F1 scores from all the y_preds.\n",
        "    y_blind is a 1D array. y_preds is a 2D array.\n",
        "    \"\"\"\n",
        "    accs = []\n",
        "    for y_pred in y_preds:\n",
        "        PRED['Facies'] = y_pred\n",
        "        all_data = PRED.join(TRUE, how='inner')\n",
        "        y_blind = all_data['LithCode'].values\n",
        "        y_pred = all_data['Facies'].values\n",
        "        y_pred = y_pred[y_blind!=11]\n",
        "        y_blind = y_blind[y_blind!=11]\n",
        "        cv_conf = confusion_matrix(y_blind, y_pred)\n",
        "        #sns.heatmap(cv_conf, annot=True, fmt=\"d\")\n",
        "        accs.append(accuracy(cv_conf))\n",
        "    return np.array(accs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bwiboVWky_1s",
        "colab_type": "code",
        "outputId": "29762bd2-e523-4573-8433-44f8032bd30c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        }
      },
      "source": [
        "from os import path\n",
        "import operator\n",
        "\n",
        "\n",
        "def accuracy(conf):\n",
        "    total_correct = 0.\n",
        "    nb_classes = conf.shape[0]\n",
        "    for i in np.arange(0,nb_classes):\n",
        "        total_correct += conf[i][i]\n",
        "    acc = total_correct/sum(sum(conf))\n",
        "    return acc\n",
        "\n",
        "scores, medians = {}, {}\n",
        "for f in ['LA_Team_100_realizations.npy']:\n",
        "    team = path.basename(f).split('_')[0]\n",
        "    y_preds = np.load(f)\n",
        "    scores[team] = get_accuracies(y_preds)\n",
        "    medians[team] = np.median(scores[team])\n",
        "    plt.hist(pd.Series(scores[team]), alpha=0.5)\n",
        "\n",
        "for t, m in sorted(medians.items(), key=operator.itemgetter(1), reverse=True):\n",
        "    print(\"{:20s}{:.4f}\".format(t, m))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "LA                  0.6412\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAAD8CAYAAABXe05zAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ne53kLRikKG5dAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}