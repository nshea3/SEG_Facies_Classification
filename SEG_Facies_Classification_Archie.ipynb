{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SEG_Facies_Classification_Archie.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nshea3/SEG_Facies_Classification/blob/master/SEG_Facies_Classification_Archie.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Yl6Z7Ytsfgk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "data = pd.read_csv('https://raw.githubusercontent.com/seg/2016-ml-contest/master/training_data.csv')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jv1j1ztw_GoL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import KFold , StratifiedKFold\n",
        "from sklearn.metrics import confusion_matrix, f1_score\n",
        "from sklearn import preprocessing\n",
        "from sklearn.model_selection import LeavePGroupsOut\n",
        "from sklearn.multiclass import OneVsOneClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from scipy.signal import medfilt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c9QpsHDtsrYl",
        "colab_type": "code",
        "outputId": "8eadd5d6-8def-496d-e7a4-ea3a23c35b52",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "#Load Data\n",
        "data = pd.read_csv('https://raw.githubusercontent.com/seg/2016-ml-contest/master/facies_vectors.csv')\n",
        "\n",
        "data['logF/logphi'] = data['ILD_log10'] / np.log10(data['PHIND'])\n",
        "\n",
        "# Parameters\n",
        "feature_names = ['GR', 'ILD_log10', 'DeltaPHI', 'PHIND', 'PE', 'NM_M', 'RELPOS', 'logF/logphi']\n",
        "facies_names = ['SS', 'CSiS', 'FSiS', 'SiSh', 'MS', 'WS', 'D', 'PS', 'BS']\n",
        "facies_colors = ['#F4D03F', '#F5B041','#DC7633','#6E2C00', '#1B4F72','#2E86C1', '#AED6F1', '#A569BD', '#196F3D']\n",
        "\n",
        "# Store features and labels\n",
        "X = data[feature_names].values \n",
        "y = data['Facies'].values \n",
        "\n",
        "# Store well labels and depths\n",
        "well = data['Well Name'].values\n",
        "depth = data['Depth'].values\n",
        "\n",
        "# Fill 'PE' missing values with mean\n",
        "imp = preprocessing.Imputer(missing_values='NaN', strategy='mean', axis=0)\n",
        "imp.fit(X)\n",
        "X = imp.transform(X)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:66: DeprecationWarning: Class Imputer is deprecated; Imputer was deprecated in version 0.20 and will be removed in 0.22. Import impute.SimpleImputer from sklearn instead.\n",
            "  warnings.warn(msg, category=DeprecationWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ax22uIvcstbT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Feature windows concatenation function\n",
        "def augment_features_window(X, N_neig):\n",
        "    \n",
        "    # Parameters\n",
        "    N_row = X.shape[0]\n",
        "    N_feat = X.shape[1]\n",
        "\n",
        "    # Zero padding\n",
        "    X = np.vstack((np.zeros((N_neig, N_feat)), X, (np.zeros((N_neig, N_feat)))))\n",
        "\n",
        "    # Loop over windows\n",
        "    X_aug = np.zeros((N_row, N_feat*(2*N_neig+1)))\n",
        "    for r in np.arange(N_row)+N_neig:\n",
        "        this_row = []\n",
        "        for c in np.arange(-N_neig,N_neig+1):\n",
        "            this_row = np.hstack((this_row, X[r+c]))\n",
        "        X_aug[r-N_neig] = this_row\n",
        "\n",
        "    return X_aug\n",
        "\n",
        "\n",
        "# Feature gradient computation function\n",
        "def augment_features_gradient(X, depth):\n",
        "    \n",
        "    # Compute features gradient\n",
        "    d_diff = np.diff(depth).reshape((-1, 1))\n",
        "    d_diff[d_diff==0] = 0.001\n",
        "    X_diff = np.diff(X, axis=0)\n",
        "    X_grad = X_diff / d_diff\n",
        "        \n",
        "    # Compensate for last missing value\n",
        "    X_grad = np.concatenate((X_grad, np.zeros((1, X_grad.shape[1]))))\n",
        "    \n",
        "    return X_grad\n",
        "\n",
        "\n",
        "# Feature augmentation function\n",
        "def augment_features(X, well, depth, N_neig=1):\n",
        "    \n",
        "    # Augment features\n",
        "    X_aug = np.zeros((X.shape[0], X.shape[1]*(N_neig*2+2)))\n",
        "    for w in np.unique(well):\n",
        "        w_idx = np.where(well == w)[0]\n",
        "        X_aug_win = augment_features_window(X[w_idx, :], N_neig)\n",
        "        X_aug_grad = augment_features_gradient(X[w_idx, :], depth[w_idx])\n",
        "        X_aug[w_idx, :] = np.concatenate((X_aug_win, X_aug_grad), axis=1)\n",
        "    \n",
        "    # Find padded rows\n",
        "    padded_rows = np.unique(np.where(X_aug[:, 0:7] == np.zeros((1, 7)))[0])\n",
        "    \n",
        "    return X_aug, padded_rows"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2PJXjTrDsxa3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_aug, padded_rows = augment_features(X, well, depth)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dbUKAaW4sz6v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def preprocess():\n",
        "    \n",
        "    # Preprocess data to use in model\n",
        "    X_train_aux = []\n",
        "    X_test_aux = []\n",
        "    y_train_aux = []\n",
        "    y_test_aux = []\n",
        "    \n",
        "    # For each data split\n",
        "    split = split_list[5]\n",
        "        \n",
        "    # Remove padded rows\n",
        "    split_train_no_pad = np.setdiff1d(split['train'], padded_rows)\n",
        "\n",
        "    # Select training and validation data from current split\n",
        "    X_tr = X_aug[split_train_no_pad, :]\n",
        "    X_v = X_aug[split['val'], :]\n",
        "    y_tr = y[split_train_no_pad]\n",
        "    y_v = y[split['val']]\n",
        "\n",
        "    # Select well labels for validation data\n",
        "    well_v = well[split['val']]\n",
        "\n",
        "    # Feature normalization\n",
        "    scaler = preprocessing.RobustScaler(quantile_range=(25.0, 75.0)).fit(X_tr)\n",
        "    X_tr = scaler.transform(X_tr)\n",
        "    X_v = scaler.transform(X_v)\n",
        "        \n",
        "    X_train_aux.append( X_tr )\n",
        "    X_test_aux.append( X_v )\n",
        "    y_train_aux.append( y_tr )\n",
        "    y_test_aux.append (  y_v )\n",
        "    \n",
        "    X_train = np.concatenate( X_train_aux )\n",
        "    X_test = np.concatenate ( X_test_aux )\n",
        "    y_train = np.concatenate ( y_train_aux )\n",
        "    y_test = np.concatenate ( y_test_aux )\n",
        "    \n",
        "    return X_train , X_test , y_train , y_test"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qbs7bPYEs_v6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.ensemble import  RandomForestClassifier, VotingClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import BernoulliNB\n",
        "from sklearn.pipeline import make_pipeline, make_union\n",
        "from sklearn.preprocessing import FunctionTransformer\n",
        "import xgboost as xgb\n",
        "from xgboost.sklearn import  XGBClassifier"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JSi9gc6MtCTX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_and_test(X_tr, y_tr, X_v, well_v, clf):\n",
        "    \n",
        "    # Feature normalization\n",
        "    scaler = preprocessing.RobustScaler(quantile_range=(25.0, 75.0)).fit(X_tr)\n",
        "    X_tr = scaler.transform(X_tr)\n",
        "    X_v = scaler.transform(X_v)\n",
        "    \n",
        "    clf.fit(X_tr, y_tr)\n",
        "    \n",
        "    # Test classifier\n",
        "    y_v_hat = clf.predict(X_v)\n",
        "    \n",
        "    # Clean isolated facies for each well\n",
        "    for w in np.unique(well_v):\n",
        "        y_v_hat[well_v==w] = medfilt(y_v_hat[well_v==w], kernel_size=5)\n",
        "    \n",
        "    return y_v_hat"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l8EGh1wLtFp1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_data = pd.read_csv('https://raw.githubusercontent.com/seg/2016-ml-contest/master/validation_data_nofacies.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R2eZebobtRpM",
        "colab_type": "code",
        "outputId": "afd24cdd-556a-454f-9a7d-90eb4e655f2a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "#Load testing data\n",
        "\n",
        "    # Train classifier\n",
        "    #clf = make_pipeline(make_union(VotingClassifier([(\"est\", ExtraTreesClassifier(criterion=\"gini\", max_features=1.0, n_estimators=500))]), FunctionTransformer(lambda X: X)), XGBClassifier(learning_rate=0.73, max_depth=10, min_child_weight=10, n_estimators=500, subsample=0.27))\n",
        "    #clf =  make_pipeline( KNeighborsClassifier(n_neighbors=5, weights=\"distance\") ) \n",
        "    #clf = make_pipeline(MaxAbsScaler(),make_union(VotingClassifier([(\"est\", RandomForestClassifier(n_estimators=500))]), FunctionTransformer(lambda X: X)),ExtraTreesClassifier(criterion=\"entropy\", max_features=0.0001, n_estimators=500))\n",
        "    # * clf = make_pipeline( make_union(VotingClassifier([(\"est\", BernoulliNB(alpha=60.0, binarize=0.26, fit_prior=True))]), FunctionTransformer(lambda X: X)),RandomForestClassifier(n_estimators=500))\n",
        "\n",
        "# # Prepare training data\n",
        "# X_tr = X\n",
        "# y_tr = y\n",
        "\n",
        "# # Augment features\n",
        "# X_tr, padded_rows = augment_features(X_tr, well, depth)\n",
        "\n",
        "# # Removed padded rows\n",
        "# X_tr = np.delete(X_tr, padded_rows, axis=0)\n",
        "# y_tr = np.delete(y_tr, padded_rows, axis=0) \n",
        "\n",
        "# Prepare test data\n",
        "\n",
        "test_data['logF/logphi'] = test_data['ILD_log10'] / np.log10(test_data['PHIND'])\n",
        "\n",
        "well_ts = test_data['Well Name'].values\n",
        "depth_ts = test_data['Depth'].values\n",
        "X_ts = test_data[feature_names].values\n",
        "\n",
        "\n",
        "    \n",
        "y_pred = []\n",
        "print('.' * 20)\n",
        "for seed in range(20):\n",
        "    np.random.seed(seed)\n",
        "\n",
        "    # Make training data.\n",
        "    X_train, padded_rows = augment_features(X, well, depth)\n",
        "    y_train = y\n",
        "    X_train = np.delete(X_train, padded_rows, axis=0)\n",
        "    y_train = np.delete(y_train, padded_rows, axis=0) \n",
        "\n",
        "    # Train classifier  \n",
        "    clf = make_pipeline(XGBClassifier(learning_rate=0.12,\n",
        "                                      max_depth=5,\n",
        "                                      min_child_weight=10,\n",
        "                                      n_estimators=175,\n",
        "                                      seed=seed,\n",
        "                                      colsample_bytree=0.9))\n",
        "\n",
        "    # Make blind data.\n",
        "    X_test, _ = augment_features(X_ts, well_ts, depth_ts)\n",
        "\n",
        "    # Train and test.\n",
        "    y_ts_hat = train_and_test(X_train, y_train, X_test, well_ts, clf)\n",
        "    \n",
        "    # Collect result.\n",
        "    y_pred.append(y_ts_hat)\n",
        "    print('|', end='')\n",
        "    \n",
        "np.save('LA_Team_100_realizations.npy', y_pred)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "....................\n",
            "||||||||||||||||||||"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2BDIRiQ4yMus",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "PRED = pd.read_csv('https://raw.githubusercontent.com/seg/2016-ml-contest/master/prediction_depths.csv')\n",
        "PRED.set_index([\"Well Name\", \"Depth\"], inplace=True)\n",
        "PRED.head()\n",
        "\n",
        "TRUE = pd.read_csv('https://raw.githubusercontent.com/seg/2016-ml-contest/master/blind_stuart_crawford_core_facies.csv')\n",
        "TRUE.rename(columns={'Depth.ft': 'Depth'}, inplace=True)\n",
        "TRUE.rename(columns={'WellName': 'Well Name'}, inplace=True)\n",
        "TRUE.set_index([\"Well Name\", \"Depth\"], inplace=True)\n",
        "TRUE.head()\n",
        "\n",
        "\n",
        "def get_accuracies(y_preds):\n",
        "    \"\"\"\n",
        "    Get the F1 scores from all the y_preds.\n",
        "    y_blind is a 1D array. y_preds is a 2D array.\n",
        "    \"\"\"\n",
        "    accs = []\n",
        "    for y_pred in y_preds:\n",
        "        PRED['Facies'] = y_pred\n",
        "        all_data = PRED.join(TRUE, how='inner')\n",
        "        y_blind = all_data['LithCode'].values\n",
        "        y_pred = all_data['Facies'].values\n",
        "        y_pred = y_pred[y_blind!=11]\n",
        "        y_blind = y_blind[y_blind!=11]\n",
        "        cv_conf = confusion_matrix(y_blind, y_pred)\n",
        "        #sns.heatmap(cv_conf, annot=True, fmt=\"d\")\n",
        "        accs.append(accuracy(cv_conf))\n",
        "    return np.array(accs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bwiboVWky_1s",
        "colab_type": "code",
        "outputId": "9d63c37f-1c27-4bd5-92c9-8c176e82fe55",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        }
      },
      "source": [
        "from os import path\n",
        "import operator\n",
        "\n",
        "\n",
        "def accuracy(conf):\n",
        "    total_correct = 0.\n",
        "    nb_classes = conf.shape[0]\n",
        "    for i in np.arange(0,nb_classes):\n",
        "        total_correct += conf[i][i]\n",
        "    acc = total_correct/sum(sum(conf))\n",
        "    return acc\n",
        "\n",
        "scores, medians = {}, {}\n",
        "for f in ['LA_Team_100_realizations.npy']:\n",
        "    team = path.basename(f).split('_')[0]\n",
        "    y_preds = np.load(f)\n",
        "    scores[team] = get_accuracies(y_preds)\n",
        "    medians[team] = np.median(scores[team])\n",
        "    plt.hist(pd.Series(scores[team]), alpha=0.5)\n",
        "\n",
        "for t, m in sorted(medians.items(), key=operator.itemgetter(1), reverse=True):\n",
        "    print(\"{:20s}{:.4f}\".format(t, m))"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "LA                  0.6350\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAAD8CAYAAABXe05zAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nMMINMIxwAwzz/0AZEjJZQAkoAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}